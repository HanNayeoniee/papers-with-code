{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from typing import Tuple\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, ffn_hidden, n_head, drop_prob):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.attention = MultiHeadAttention(d_model=d_model, n_head=n_head)\n",
        "        self.norm1 = LayerNorm(d_model=d_model)\n",
        "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
        "        self.norm2 = LayerNorm(d_model=d_model)\n",
        "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, x, src_mask):\n",
        "        # 1. compute self attention\n",
        "        _x = x\n",
        "        x = self.attention(q=x, k=x, v=x, mask=src_mask)\n",
        "        \n",
        "        # 2. add and norm\n",
        "        x = self.norm1(x + _x)\n",
        "        x = self.dropout1(x)\n",
        "        \n",
        "        # 3. positionwise feed forward network\n",
        "        _x = x\n",
        "        x = self.ffn(x)\n",
        "      \n",
        "        # 4. add and norm\n",
        "        x = self.norm2(x + _x)\n",
        "        x = self.dropout2(x)\n",
        "        return x\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, n_head):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.n_head = n_head\n",
        "        self.attention = ScaleDotProductAttention()\n",
        "        self.w_q = nn.Linear(d_model, d_model)\n",
        "        self.w_k = nn.Linear(d_model, d_model)\n",
        "        self.w_v = nn.Linear(d_model, d_model)\n",
        "        self.w_concat = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        # 1. dot product with weight matrices\n",
        "        q, k, v = self.w_q(q), self.w_k(k), self.w_v(v)\n",
        "\n",
        "        # 2. split tensor by number of heads\n",
        "        q, k, v = self.split(q), self.split(k), self.split(v)\n",
        "\n",
        "        # 3. do scale dot product to compute similarity\n",
        "        out, attention = self.attention(q, k, v, mask=mask)\n",
        "        \n",
        "        # 4. concat and pass to linear layer\n",
        "        out = self.concat(out)\n",
        "        out = self.w_concat(out)\n",
        "\n",
        "        # 5. visualize attention map\n",
        "        # TODO : we should implement visualization\n",
        "\n",
        "        return out\n",
        "\n",
        "    def split(self, tensor):\n",
        "        \"\"\"\n",
        "        split tensor by number of head\n",
        "\n",
        "        :param tensor: [batch_size, length, d_model]\n",
        "        :return: [batch_size, head, length, d_tensor]\n",
        "        \"\"\"\n",
        "        batch_size, length, d_model = tensor.size()\n",
        "\n",
        "        d_tensor = d_model // self.n_head\n",
        "        tensor = tensor.view(batch_size, length, self.n_head, d_tensor).transpose(1, 2)\n",
        "        # it is similar with group convolution (split by number of heads)\n",
        "\n",
        "        return tensor\n",
        "\n",
        "    def concat(self, tensor):\n",
        "        \"\"\"\n",
        "        inverse function of self.split(tensor : torch.Tensor)\n",
        "\n",
        "        :param tensor: [batch_size, head, length, d_tensor]\n",
        "        :return: [batch_size, length, d_model]\n",
        "        \"\"\"\n",
        "        batch_size, head, length, d_tensor = tensor.size()\n",
        "        d_model = head * d_tensor\n",
        "\n",
        "        tensor = tensor.transpose(1, 2).contiguous().view(batch_size, length, d_model)\n",
        "        return tensor\n",
        "\n",
        "class ScaleDotProductAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    compute scale dot product attention\n",
        "\n",
        "    Query : given sentence that we focused on (decoder)\n",
        "    Key : every sentence to check relationship with Qeury(encoder)\n",
        "    Value : every sentence same with Key (encoder)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ScaleDotProductAttention, self).__init__()\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None, e=1e-12):\n",
        "        # input is 4 dimension tensor\n",
        "        # [batch_size, head, length, d_tensor]\n",
        "        batch_size, head, length, d_tensor = k.size()\n",
        "\n",
        "        # 1. dot product Query with Key^T to compute similarity\n",
        "        k_t = k.transpose(2, 3)  # transpose\n",
        "        score = (q @ k_t) / math.sqrt(d_tensor)  # scaled dot product\n",
        "\n",
        "        # 2. apply masking (opt)\n",
        "        if mask is not None:\n",
        "            print(score.shape)\n",
        "            score = score.masked_fill(mask == 0, -e)\n",
        "\n",
        "        # 3. pass them softmax to make [0, 1] range\n",
        "        score = self.softmax(score)\n",
        "\n",
        "        # 4. multiply with Value\n",
        "        v = score @ v\n",
        "\n",
        "        return v, score\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, d_model, eps=1e-12):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
        "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        # '-1' means last dimension. \n",
        "\n",
        "        out = (x - mean) / (std + self.eps)\n",
        "        out = self.gamma * out + self.beta\n",
        "        return out\n",
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, hidden)\n",
        "        self.linear2 = nn.Linear(hidden, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear2(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "nm3laXW5ppib"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    BERT Embedding which is consisted with under features\n",
        "        1. TokenEmbedding : normal embedding matrix\n",
        "        2. PositionalEmbedding : adding positional information using sin, cos\n",
        "        2. SegmentEmbedding : adding sentence segment info, (sent_A:1, sent_B:2)\n",
        "        sum of all these features are output of BERTEmbedding\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embed_size, dropout=0.1):\n",
        "        \"\"\"\n",
        "        :param vocab_size: total vocab size\n",
        "        :param embed_size: embedding size of token embedding\n",
        "        :param dropout: dropout rate\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.token = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
        "        self.position = PositionalEmbedding(embed_size)\n",
        "        self.segment = nn.Embedding(2, embed_size)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.embed_size = embed_size\n",
        "\n",
        "    def forward(self, sequence, segment_label):\n",
        "        x = self.token(sequence) + self.position(sequence) + self.segment(segment_label)\n",
        "        return self.dropout(x)\n",
        "\n",
        "class PositionalEmbedding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, max_len=512):\n",
        "        super().__init__()\n",
        "\n",
        "        # Compute the positional encodings once in log space.\n",
        "        pe = torch.zeros(max_len, d_model).float()\n",
        "        pe.require_grad = False\n",
        "\n",
        "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
        "        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.pe[:, :x.size(1)]"
      ],
      "metadata": {
        "id": "iK-x7EN70IM3"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BERT(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_dim: int,\n",
        "                 emb_dim: int,\n",
        "                 n_layers: int,\n",
        "                 ffn_hidden: int,\n",
        "                 n_head: int,\n",
        "                 dropout: float,\n",
        "                 device\n",
        "                 ):\n",
        "        \"\"\"\n",
        "        The encoder part of Transformer.\n",
        "\n",
        "        Args:\n",
        "            input_dim: the dimension of input\n",
        "            emb_dim: the dimension of embedding vector\n",
        "            n_layers: the number of encoder layers\n",
        "            dropout: dropout ratio\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.emb_dim = emb_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.embedding = BERTEmbedding(self.input_dim, self.emb_dim)\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model=emb_dim,\n",
        "                                                  ffn_hidden=ffn_hidden,\n",
        "                                                  n_head=n_head,\n",
        "                                                  drop_prob=self.dropout\n",
        "                                                  ) for _ in range(n_layers)])\n",
        "    def forward(self, x, segment_info):\n",
        "        x = self.embedding(x, segment_info)\n",
        "        x = self.pos_emb(x)\n",
        "\n",
        "        mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "6jMJk_wNCkFi"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "### BERT(BASE) HYPER PARAMETERS ###\n",
        "# INPUT_DIM = 30000\n",
        "# ENC_EMB_DIM = 768\n",
        "# N_LAYERS = 12\n",
        "# FFN_HIDDEN_DIM = 4 * ENC_EMB_DIM\n",
        "# N_HEADS = 12\n",
        "# ENC_DROPOUT = 0.1\n",
        "\n",
        "### BERT(LARGE) HYPER PARAMETERS ###\n",
        "INPUT_DIM = 30000\n",
        "ENC_EMB_DIM = 1024\n",
        "N_LAYERS = 24\n",
        "FFN_HIDDEN_DIM = 4 * ENC_EMB_DIM\n",
        "N_HEADS = 16\n",
        "ENC_DROPOUT = 0.1\n",
        "\n",
        "\n",
        "\n",
        "bert = BERT(INPUT_DIM, ENC_EMB_DIM, N_LAYERS, FFN_HIDDEN_DIM, N_HEADS, ENC_DROPOUT, device)\n",
        "\n",
        "def count_parameters(model: nn.Module):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(bert):,} trainable parameters')"
      ],
      "metadata": {
        "id": "mWPtYkPf0YFo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b4e1a7f-7aef-4fe2-af3c-f0afb1dfc396"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 333,031,424 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BkWE1OXTAbIy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}